<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="URF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
    <style>
        canvas, video {
            position: absolute;
            right: 400;
            top: 200;
        }


    </style>
</head>

<body onload="init()">
    <h1>Hello Welt</h1>
    <video id="video" width="720" height="480" autoplay muted>
    <script src="face-api.js"></script>
    <script>
        async function init() {
            await faceapi.nets.tinyFaceDetector.loadFromUri('/models')
            await faceapi.nets.faceLandmark68Net.loadFromUri('/models')
            await faceapi.nets.faceRecognitionNet.loadFromUri('/models')
            await faceapi.nets.faceExpressionNet.loadFromUri('/models')
            navigator.getUserMedia({
                    video: {}
                },
                s => video.srcObject = s, 
                console.error );
        }

        video.addEventListener('play', startFaceDetection)

        function startFaceDetection(){
            let canvas = faceapi.createCanvasFromMedia(video);
            document.body.append(canvas);

            setInterval(async function(){
                let options = new faceapi.TinyFaceDetectorOptions();
                canvas.getContext('2d').clearRect(0, 0, canvas.width, canvas.height)
                
                let size = {width: video.width, height: video.height}
                let detections = await faceapi.detectAllFaces(video, options).withFaceLandmarks().withFaceExpressions();
                let resizeDetections = faceapi.resizeResults(detections, size);
                faceapi.draw.drawDetections(canvas, resizeDetections);
                faceapi.draw.drawFaceLandmarks(canvas, resizeDetections);
                faceapi.draw.drawFaceExpressions(canvas, resizeDetections);
               }, 250);
        }
    </script>
</body>

</html>